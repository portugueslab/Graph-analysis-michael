{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import flammkuchen as fl\n",
    "import napari\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from split_dataset import SplitDataset\n",
    "import json\n",
    "\n",
    "from bg_atlasapi.bg_atlas import BrainGlobeAtlas\n",
    "from bg_space import AnatomicalSpace\n",
    "\n",
    "from quickdisplay import *\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210721_f1'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210721_f2'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210721_f3'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210722_f0'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210722_f2'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210722_f4'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210722_f5'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210722_f6'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210723_f1'),\n",
       " WindowsPath('//FUNES/Shared/experiments/E0054_velocity_integrators/v05_imaging_vel_vs_freq/210723_f2')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(r\"\\\\FUNES\\Shared\\experiments\\E0054_velocity_integrators\\v05_imaging_vel_vs_freq\")\n",
    "path_list = list((data_dir).glob('*f[0-9]*'))\n",
    "path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reference stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify stack to use as reference, and import stack and metadata to define image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:  \\\\FUNES\\Shared\\experiments\\E0054_velocity_integrators\\v05_imaging_vel_vs_freq\\210722_f0\n"
     ]
    }
   ],
   "source": [
    "###FUNCTIONAL IMAGING STACK###\n",
    "\n",
    "#Specify fish to use as reference\n",
    "ref_fish = 3\n",
    "ref_path = path_list[ref_fish]\n",
    "print('Reference: ', ref_path)\n",
    "\n",
    "ref = SplitDataset(Path(ref_path / 'anatomy'))[1:, :, :]\n",
    "\n",
    "#Retrieve resolution\n",
    "with open(next((ref_path / 'behavior').glob('*metadata*')), \"r\") as f:\n",
    "        ref_metadata = json.load(f)\n",
    "\n",
    "ls_config = ref_metadata['imaging']['microscope_config']['lightsheet']['scanning']\n",
    "z_tot_span = ls_config[\"z\"][\"piezo_max\"] - ls_config[\"z\"][\"piezo_min\"]\n",
    "n_planes = ls_config[\"triggering\"][\"n_planes\"]\n",
    "\n",
    "ref_res = [z_tot_span / n_planes, 0.6, 0.6] #Resolution for xy axis in the LS is 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BRAINRENDER REFERENCE###\n",
    "\n",
    "#Load MPI reference stack\n",
    "mpi_ref = BrainGlobeAtlas('mpin_zfish_1um')\n",
    "ref = mpi_ref.additional_references['H2BGCaMP']\n",
    "\n",
    "#Store resolution\n",
    "ref_res = mpi_ref.metadata['resolution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load moving stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify fish to register to the reference stack. Directory will be used as output for the resampled images and initial transformation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphing:  \\\\FUNES\\Shared\\experiments\\E0054_velocity_integrators\\v05_imaging_vel_vs_freq\\210722_f5\n"
     ]
    }
   ],
   "source": [
    "#Specify fish to register\n",
    "fish_path = path_list[6]\n",
    "print('Morphing: ', fish_path)\n",
    "\n",
    "#Load functional dataset to align\n",
    "mov = SplitDataset(Path(fish_path / 'anatomy'))[1:, :, :]\n",
    "\n",
    "#Retrieve resolution\n",
    "with open(next((fish_path / 'behavior').glob('*metadata*')), \"r\") as f:\n",
    "        fish_metadata = json.load(f)\n",
    "\n",
    "mov_config = fish_metadata['imaging']['microscope_config']['lightsheet']['scanning']\n",
    "z_tot_span = mov_config[\"z\"][\"piezo_max\"] - mov_config[\"z\"][\"piezo_min\"]\n",
    "n_planes = mov_config[\"triggering\"][\"n_planes\"]\n",
    "\n",
    "mov_res = [z_tot_span / n_planes, 0.6, 0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to make sure both stacks are in the same common space. When regsitering reference brains you may want to downsample their resolutions a bit as mapping between spaces later might be very time-consuming, and not all the resolution information is needed. However, when working with functional stacks, you probably don't want to do any downsampling (especially in z) to preserve as much information as possible.\n",
    "\n",
    "The most important part is to make sure you define the correct `origin` when creating the `AnatomicaSpace` objects. This will change between setups or depending on the orientation of your stacks. You can check the documentation of `bg-space` [here](https://github.com/brainglobe/bg-space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_as = AnatomicalSpace('ilp', resolution=ref_res, shape=ref.shape) #LS lab stacks\n",
    "ref_as = AnatomicalSpace(mpi_ref.orientation, resolution=ref_res, shape=ref.shape) #Brainglobe ref\n",
    "\n",
    "mov_as = AnatomicalSpace('ilp', resolution=mov_res, shape=mov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a morphing space, and map both of our stacks into it. In this case, despite you could keep the `origin` of your morphing space to be the same as your functional stacks, I suggest sticking to the `'rai'` one if you plan on doing the morphing with ANTsPy. ANTsPy works with [ITK coordinate space](https://www.slicer.org/wiki/Coordinate_systems), so sticking to this option might make your life easier on the long run.\n",
    "\n",
    "I also suggest upsampling the resolution along the z dimension. This will generate new planes by interpolating the info between adjacent planes. Might not make a lot of sense since it will make everything more computationally-intensive, but in my hands, ANTsPy registration always worked better when passing to it stacks upsampled across the z dimension. (I made this observation when starting to play with morphing parameters, so not 100% convinced about it. Would be happy to know how it works on other people's hands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define morphing space...\n",
    "morphing_as = AnatomicalSpace('rai', resolution=(1,1,1))\n",
    "\n",
    "#... and transform references to morphing space\n",
    "ref_mapped = ref_as.map_stack_to(morphing_as, ref)\n",
    "mov_mapped = mov_as.map_stack_to(morphing_as, mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load the coordinates for our ROIs and map them to the same morphing space. This is essential if you want the final registrations to be applied properly to your ROI coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ROI coordinates...\n",
    "mov_roi_coords = fl.load(fish_path / 'traces.h5')['coords']\n",
    "\n",
    "#...and map them as well to the morphing space\n",
    "mov_roi_coords_mapped = mov_as.map_points_to(morphing_as, mov_roi_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety check\n",
    "We can check that both stacks and points are in the same space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d4a864286b4684b746273b6e7d72a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='depth', step=5), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function quickdisplay.plot_side_to_side(stack1, stack2, depth, dim, stack1_title='Stack 1', stack2_title='Stack 2')>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_side_to_side,\n",
    "        stack1=fixed(ref_mapped),\n",
    "        stack2=fixed(mov_mapped),\n",
    "        depth = widgets.IntSlider(min=0, max=100, step=5, continuous_update=False),\n",
    "        dim=fixed(2), \n",
    "        stack1_title=fixed('Ref fish'),\n",
    "        stack2_title=fixed('Functional stack'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f13bbf70e7243e8b1413ef546808ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mov ROIs')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(np.nanmean(ref_mapped, 2), cmap='gray_r', origin='lower') \n",
    "axes[0].set_title('Ref stack')\n",
    "\n",
    "axes[1].imshow(np.nanmean(mov_mapped, 2), cmap='gray_r', origin='lower') \n",
    "axes[1].scatter(mov_roi_coords_mapped[:, 1], mov_roi_coords_mapped[:, 0], c='red', alpha=.0075)\n",
    "axes[1].set_title('Mov ROIs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_dir = fish_path / 'registration' / 'antspy'\n",
    "\n",
    "if not os.path.isdir(registration_dir):\n",
    "    os.mkdir(registration_dir)\n",
    "    \n",
    "#Store mapped stacks\n",
    "fl.save(registration_dir / \"ref_mapped.h5\", ref_mapped)\n",
    "fl.save(registration_dir / \"mov_mapped.h5\", mov_mapped)\n",
    "\n",
    "#Store mapped ROI coordinates\n",
    "fl.save(registration_dir / \"mov_roi_coords_mapped.h5\", mov_roi_coords_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual affine transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by finding an initial [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) based on manually-defined points. This is useful in order to get an initial rough estimate of the necessary transformation matrix needed to merge our datasets.\n",
    "\n",
    "To do so, we will open the reference and the dataset stacks in napari, and [draw some points](https://napari.org/tutorials/fundamentals/points.html) in equivalent regions of the anatomies. Start always with identifiable regions/structures in the anatomy with the lowest resolution, and find these same spots in the reference anatomy. Some good areas to try:\n",
    "- Extreme brain regions (top-most, rostral telencephalon...)\n",
    "- Well-defined structures (inferior olive, IPN, Mauthner cells...)\n",
    "- Centers or borders of larger structures (Habenulae, Optic tectums)\n",
    "- ...\n",
    "\n",
    "Make sure you add points in the two different stacks in the same order, or the resulting transformation matrix will make no sense. <br>\n",
    "And don't go crazy adding infinite points: ~6-8 points scattered around the brain should be more than enough to get a decent initial estimate.\n",
    "\n",
    "***UPDATE** I realized I prefer to go crazy adding infinite points. Whatever it takes for my initial guess to decently register regions within the brain as well as the general contour of the brain at different depths. Not sure how achievable it is to do both things simultaneously with only this tool, but honnestly I think t will save a bunch of time trying to optimize parameters in ANTsPy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_stack, mov_stack = ref_mapped, mov_mapped\n",
    "\n",
    "#Open stacks in napari\n",
    "functional_viewer = napari.view_image(mov_stack)\n",
    "reference_viewer = napari.view_image(ref_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we retrieve the points we just defined...\n",
    "functional_points = functional_viewer.layers[1].data\n",
    "reference_points = reference_viewer.layers[1].data\n",
    "\n",
    "X = np.pad(reference_points, ((0,0), (0,1)), mode=\"constant\", constant_values=1)\n",
    "Y = functional_points\n",
    "\n",
    "#...and we use some math magic to perform an initial fit \n",
    "#(this line performs some sort of lineal regression to find a transform matrix between our two arrays of points)\n",
    "transform_mat =  (np.linalg.pinv(X.T @ X) @ X.T @ Y).T\n",
    "\n",
    "#Apply transform\n",
    "transformed = map_affine(mov_stack, transform_mat, ref_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize initial approximation\n",
    "viewer = napari.view_image(ref_stack,  colormap=\"green\")\n",
    "viewer.add_image(transformed, colormap=\"magenta\", blending=\"additive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the initial transform if happy\n",
    "fl.save(registration_dir / \"initial_transform_mapped.h5\", transform_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTsPy Registration¶\n",
    "Now you should proceed onto the ANTsPy registration. To do so, open the next notebook in the repository (**2_antspy_morphing_linux**) with Jupyter Lab/Jupyter Notebook in your linux terminal. Then load the files you have generated in this notebook to your linux virtual subsystem, and follow the instructions of the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
